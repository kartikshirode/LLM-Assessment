{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad699a26",
   "metadata": {},
   "source": [
    "# 01 Data Preprocessing\n",
    "\n",
    "This notebook covers the initial data loading and preprocessing of the employee messages dataset (test.csv). We'll examine the data structure, clean the text data, and prepare it for sentiment analysis and further tasks as outlined in the project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0c2b5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import pandas, numpy, and any other libraries needed for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576effe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088916b",
   "metadata": {},
   "source": [
    "## 2. Load Raw Dataset\n",
    "Load the raw dataset from the `data/raw/` directory using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual dataset\n",
    "raw_data_path = os.path.join('..', 'data', 'raw', 'test.csv')\n",
    "df = pd.read_csv(raw_data_path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdef59",
   "metadata": {},
   "source": [
    "## 3. Inspect Data Structure\n",
    "Display the first few rows, data types, and summary statistics to understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c88137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Data types and non-null counts\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Check column names and basic structure\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb89ab",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values\n",
    "Identify and handle missing values using appropriate strategies such as imputation or removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "print('Missing values per column:')\n",
    "print(missing)\n",
    "\n",
    "# Check for empty strings in text columns\n",
    "print('\\nEmpty strings in text columns:')\n",
    "for col in ['Subject', 'body']:\n",
    "    if col in df.columns:\n",
    "        empty_count = (df[col] == '').sum()\n",
    "        print(f\"{col}: {empty_count} empty strings\")\n",
    "\n",
    "# Handle missing values - drop rows where both Subject and body are missing\n",
    "df = df.dropna(subset=['Subject', 'body'], how='all')\n",
    "print(f\"\\nDataset shape after removing rows with missing Subject AND body: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7396f",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning (Text and Numeric)\n",
    "Clean text fields (e.g., remove special characters, lowercase) and process numeric columns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73662903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text columns - Subject and body\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?]', ' ', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Clean Subject and body columns\n",
    "df['Subject_clean'] = df['Subject'].apply(clean_text)\n",
    "df['body_clean'] = df['body'].apply(clean_text)\n",
    "\n",
    "# Fill missing subjects with \"(No Subject)\"\n",
    "df['Subject_clean'] = df['Subject_clean'].replace('', '(no subject)')\n",
    "\n",
    "# Combine Subject and body for comprehensive text analysis\n",
    "df['combined_text'] = df['Subject_clean'] + ' ' + df['body_clean']\n",
    "\n",
    "print(\"Text cleaning completed!\")\n",
    "print(f\"Sample cleaned text:\\n{df['combined_text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5bb0e",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "Create new features or transform existing ones to support downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process date column\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Extract email domain from 'from' column\n",
    "df['email_domain'] = df['from'].str.split('@').str[1]\n",
    "\n",
    "# Create text length features\n",
    "df['subject_length'] = df['Subject'].str.len()\n",
    "df['body_length'] = df['body'].str.len()\n",
    "df['combined_text_length'] = df['combined_text'].str.len()\n",
    "\n",
    "# Create word count features\n",
    "df['subject_word_count'] = df['Subject_clean'].str.split().str.len()\n",
    "df['body_word_count'] = df['body_clean'].str.split().str.len()\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"New columns added: {[col for col in df.columns if col not in ['Subject', 'body', 'date', 'from']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac199a40",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "Save the cleaned and preprocessed dataset to the `data/processed/` directory for use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00887080",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = os.path.join('..', 'data', 'processed', 'email_data_processed.csv')\n",
    "df.to_csv(processed_data_path, index=False)\n",
    "print(f\"Processed data saved to {processed_data_path}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Columns in processed data: {list(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
